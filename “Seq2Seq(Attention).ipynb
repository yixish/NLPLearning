{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "colab": {
      "name": "â€œSeq2Seq(Attention)",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yixish/NLPLearning/blob/master/%E2%80%9CSeq2Seq(Attention).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9YIkuxFQWDS"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as Data\n",
        "import numpy as np\n",
        "\n",
        "import random\n",
        "import math\n",
        "import time"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oH2A4ikIQWDg"
      },
      "source": [
        "Set the random seeds for reproducability."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ON-iFjcQWDg"
      },
      "source": [
        "SEED = 1234\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YeiGsd8Be4aK",
        "outputId": "d10c1f3d-24fc-44c4-e8b7-477fee0ef6d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OA60iC5KhunC"
      },
      "source": [
        "import pandas as pd \n",
        "dir = '/content/gdrive/My Drive/dataset/'\n",
        "df = pd.read_csv(dir+\"Sentiment_Extraction103/train.csv\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiMdKZebQWDq"
      },
      "source": [
        "Load the German and English spaCy models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyLs68Fle_r1"
      },
      "source": [
        "texts = df['text'].values\n",
        "selected_texts = df['selected_text'].values\n",
        "texts = texts[:512]\n",
        "selected_texts = selected_texts[:512]\n",
        "pairs = []\n",
        "for i in range(512):\n",
        "    pairs.append([texts[i],selected_texts[i]])"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yrw9UiISfFBS",
        "outputId": "a696f5ae-3580-4786-be71-7ea92c5aaa69",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-MIq8wLfFtY",
        "outputId": "47f55545-2597-4c0c-8467-890c4f3f4366",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "vocab = set()\n",
        "vocab.add('<sos>')\n",
        "vocab.add('<eos>')\n",
        "vocab.add('<pad>')\n",
        "vocab.add('<unk>')\n",
        "\n",
        "def build_vocab(vocab,texts):\n",
        "    max_l = 0\n",
        "    for text in texts:\n",
        "        words = nltk.word_tokenize(text)\n",
        "        if max_l < len(words):\n",
        "            max_l = len(words)\n",
        "        for word in words:\n",
        "            vocab.add(word)\n",
        "    print(max_l)\n",
        "build_vocab(vocab,texts)\n",
        "build_vocab(vocab,selected_texts)\n",
        "print(\"vocal size : {}\".format(len(vocab)))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "35\n",
            "32\n",
            "vocal size : 2452\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5v1GG8fpfHt5"
      },
      "source": [
        "word2idx = { word:i for i,word in enumerate(list(vocab))}\n",
        "idx2word = { i:word for i,word in enumerate(list(vocab))}                         "
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_SRJzsnfJ2s"
      },
      "source": [
        "n_step = 40\n",
        "batch_size = 128\n",
        "n_hidden = 128\n",
        "emb_dim = 100\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "def make_data(seq_data):\n",
        "    enc_input_all, dec_input_all, dec_output_all = [], [], []\n",
        "\n",
        "    def word_2_idx(word):\n",
        "        if word in vocab:\n",
        "            return word2idx[word]\n",
        "        else: \n",
        "            return word2idx['<unk>']\n",
        "\n",
        "    for seq in seq_data:\n",
        "\n",
        "        enc_input = [word_2_idx(n) for n in  nltk.word_tokenize(seq[0])]\n",
        "        enc_input.append(word2idx['<eos>'])\n",
        "        dec_input = []\n",
        "        dec_input.append(word2idx['<sos>'])\n",
        "        dec_input.extend([word_2_idx(n) for n in nltk.word_tokenize(seq[1])])\n",
        "\n",
        "        dec_output = [word_2_idx(n) for n in nltk.word_tokenize(seq[1])] \n",
        "        dec_output.append(word2idx['<eos>'])\n",
        "\n",
        "        for i in range(n_step - len(enc_input)):\n",
        "            enc_input.append(word2idx['<pad>'])\n",
        "        for i in range(n_step - len(dec_input)):\n",
        "            dec_input.append(word2idx['<pad>'])\n",
        "        for i in range(n_step - len(dec_output)):\n",
        "            dec_output.append(word2idx['<pad>'])\n",
        "\n",
        "        # enc_input_all.append(np.eye(vocab_size)[enc_input])\n",
        "        # dec_input_all.append(np.eye(vocab_size)[dec_input])\n",
        "        enc_input_all.append(enc_input)\n",
        "        dec_input_all.append(dec_input)\n",
        "        dec_output_all.append(dec_output) \n",
        "\n",
        "\n",
        "    # make tensor\n",
        "    return torch.LongTensor(enc_input_all), torch.LongTensor(dec_input_all), torch.LongTensor(dec_output_all)\n",
        "\n",
        "enc_input_all, dec_input_all, dec_output_all = make_data(pairs)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLsKqdMufSUX"
      },
      "source": [
        "class TranslateDataSet(Data.Dataset):\n",
        "    def __init__(self, enc_input_all, dec_input_all, dec_output_all):\n",
        "        self.enc_input_all = enc_input_all\n",
        "        self.dec_input_all = dec_input_all\n",
        "        self.dec_output_all = dec_output_all\n",
        "    \n",
        "    def __len__(self): # return dataset size\n",
        "        return len(self.enc_input_all)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.enc_input_all[idx], self.dec_input_all[idx], self.dec_output_all[idx]\n",
        "\n",
        "loader = Data.DataLoader(TranslateDataSet(enc_input_all, dec_input_all, dec_output_all), batch_size, True)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_No_GgVGQWEb"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# device = torch.device('cpu')"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57nfv01ZQWEf"
      },
      "source": [
        "Create the iterators."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nY7wvjYfQWEk"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True)\n",
        "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, src): \n",
        "        '''\n",
        "        src = [src_len, batch_size]\n",
        "        '''\n",
        "        src = src.transpose(0, 1) # src = [batch_size, src_len]\n",
        "\n",
        "        embedded = self.dropout(self.embedding(src)).transpose(0, 1) # embedded = [src_len, batch_size, emb_dim]\n",
        "\n",
        "        # enc_output = [src_len, batch_size, hid_dim * num_directions]\n",
        "        # enc_hidden = [n_layers * num_directions, batch_size, hid_dim]\n",
        "        enc_output, enc_hidden = self.rnn(embedded) # if h_0 is not give, it will be set 0 acquiescently\n",
        "\n",
        "        # enc_hidden is stacked [forward_1, backward_1, forward_2, backward_2, ...]\n",
        "        # enc_output are always from the last layer\n",
        "        \n",
        "        # enc_hidden [-2, :, : ] is the last of the forwards RNN \n",
        "        # enc_hidden [-1, :, : ] is the last of the backwards RNN\n",
        "        \n",
        "        # initial decoder hidden is final hidden state of the forwards and backwards \n",
        "        # encoder RNNs fed through a linear layer\n",
        "        # s = [batch_size, dec_hid_dim]\n",
        "        s = torch.tanh(self.fc(torch.cat((enc_hidden[-2,:,:], enc_hidden[-1,:,:]), dim = 1)))\n",
        "        \n",
        "        return enc_output, s"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7IJ0v66QWEv"
      },
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
        "        super().__init__()\n",
        "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim, bias=False)\n",
        "        self.v = nn.Linear(dec_hid_dim, 1, bias = False)\n",
        "        \n",
        "    def forward(self, s, enc_output):\n",
        "        \n",
        "        # s = [batch_size, dec_hid_dim]\n",
        "        # enc_output = [src_len, batch_size, enc_hid_dim * 2]\n",
        "        \n",
        "        batch_size = enc_output.shape[1]\n",
        "        src_len = enc_output.shape[0]\n",
        "        \n",
        "        # repeat decoder hidden state src_len times\n",
        "        # s = [batch_size, src_len, dec_hid_dim]\n",
        "        # enc_output = [batch_size, src_len, enc_hid_dim * 2]\n",
        "        s = s.unsqueeze(1).repeat(1, src_len, 1)\n",
        "        enc_output = enc_output.transpose(0, 1)\n",
        "        \n",
        "        # energy = [batch_size, src_len, dec_hid_dim]\n",
        "        energy = torch.tanh(self.attn(torch.cat((s, enc_output), dim = 2)))\n",
        "        \n",
        "        # attention = [batch_size, src_len]\n",
        "        attention = self.v(energy).squeeze(2)\n",
        "        \n",
        "        return F.softmax(attention, dim=1)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIvuL5awQWE0"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
        "        super().__init__()\n",
        "        self.output_dim = output_dim\n",
        "        self.attention = attention\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
        "        self.fc_out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, dec_input, s, enc_output):\n",
        "             \n",
        "        # dec_input = [batch_size]\n",
        "        # s = [batch_size, dec_hid_dim]\n",
        "        # enc_output = [src_len, batch_size, enc_hid_dim * 2]\n",
        "        \n",
        "        dec_input = dec_input.unsqueeze(1) # dec_input = [batch_size, 1]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(dec_input)).transpose(0, 1) # embedded = [1, batch_size, emb_dim]\n",
        "        \n",
        "        # a = [batch_size, 1, src_len]  \n",
        "        a = self.attention(s, enc_output).unsqueeze(1)\n",
        "        \n",
        "        # enc_output = [batch_size, src_len, enc_hid_dim * 2]\n",
        "        enc_output = enc_output.transpose(0, 1)\n",
        "\n",
        "        # c = [1, batch_size, enc_hid_dim * 2]\n",
        "        c = torch.bmm(a, enc_output).transpose(0, 1)\n",
        "\n",
        "        # rnn_input = [1, batch_size, (enc_hid_dim * 2) + emb_dim]\n",
        "        rnn_input = torch.cat((embedded, c), dim = 2)\n",
        "            \n",
        "        # dec_output = [src_len(=1), batch_size, dec_hid_dim]\n",
        "        # dec_hidden = [n_layers * num_directions, batch_size, dec_hid_dim]\n",
        "        dec_output, dec_hidden = self.rnn(rnn_input, s.unsqueeze(0))\n",
        "        \n",
        "        # embedded = [batch_size, emb_dim]\n",
        "        # dec_output = [batch_size, dec_hid_dim]\n",
        "        # c = [batch_size, enc_hid_dim * 2]\n",
        "        embedded = embedded.squeeze(0)\n",
        "        dec_output = dec_output.squeeze(0)\n",
        "        c = c.squeeze(0)\n",
        "        \n",
        "        # pred = [batch_size, output_dim]\n",
        "        pred = self.fc_out(torch.cat((dec_output, c, embedded), dim = 1))\n",
        "        \n",
        "        return pred, dec_hidden.squeeze(0)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64wJqu2TQWE4"
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "        \n",
        "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
        "        \n",
        "        # src = [src_len, batch_size]\n",
        "        # trg = [trg_len, batch_size]\n",
        "        # teacher_forcing_ratio is probability to use teacher forcing\n",
        "\n",
        "        src = src.transpose(0,1)\n",
        "        trg = trg.transpose(0,1)\n",
        "        \n",
        "        batch_size = src.shape[1]\n",
        "        trg_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "        \n",
        "        # tensor to store decoder outputs\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "\n",
        "\n",
        "        # enc_output is all hidden states of the input sequence, back and forwards\n",
        "        # s is the final forward and backward hidden states, passed through a linear layer\n",
        "        enc_output, s = self.encoder(src)\n",
        "                \n",
        "        # first input to the decoder is the <sos> tokens\n",
        "        dec_input = trg[0,:]\n",
        "        \n",
        "        for t in range(1, trg_len):\n",
        "            \n",
        "            # insert dec_input token embedding, previous hidden state and all encoder hidden states\n",
        "            # receive output tensor (predictions) and new hidden state\n",
        "            dec_output, s = self.decoder(dec_input, s, enc_output)\n",
        "            \n",
        "            # place predictions in a tensor holding predictions for each token\n",
        "            outputs[t] = dec_output\n",
        "            \n",
        "            # decide if we are going to use teacher forcing or not\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            \n",
        "            # get the highest predicted token from our predictions\n",
        "            top1 = dec_output.argmax(1) \n",
        "            \n",
        "            # if teacher forcing, use actual next token as next input\n",
        "            # if not, use predicted token\n",
        "            dec_input = trg[t] if teacher_force else top1\n",
        "\n",
        "        return outputs"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXpZLK3xQWE-"
      },
      "source": [
        "# INPUT_DIM = len(SRC.vocab)\n",
        "# OUTPUT_DIM = len(TRG.vocab)\n",
        "INPUT_DIM = vocab_size\n",
        "OUTPUT_DIM  = vocab_size\n",
        "ENC_EMB_DIM = 128\n",
        "DEC_EMB_DIM = 128\n",
        "ENC_HID_DIM = 256\n",
        "DEC_HID_DIM = 256\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "\n",
        "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
        "\n",
        "model = Seq2Seq(enc, dec, device).to(device)\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnYtPpQSfwfO",
        "outputId": "6a9d318c-9a4a-480b-f07d-e3ace21cdef8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for epoch in range(500):\n",
        "  for enc_input_batch, dec_input_batch, dec_output_batch in loader:\n",
        "      # make hidden shape [num_layers * num_directions, batch_size, n_hidden]\n",
        "    #   h_0 = torch.zeros(1, batch_size, n_hidden).to(device)\n",
        "\n",
        "      (enc_input_batch, dec_intput_batch, dec_output_batch) = (enc_input_batch.to(device), dec_input_batch.to(device), dec_output_batch.to(device))\n",
        "      # enc_input_batch : [batch_size, n_step+1, n_class]\n",
        "      # dec_intput_batch : [batch_size, n_step+1, n_class]\n",
        "      # dec_output_batch : [batch_size, n_step+1], not one-hot\n",
        "\n",
        "\n",
        "      pred = model(enc_input_batch,dec_intput_batch,0)\n",
        "      # pred : [n_step+1, batch_size, n_class]\n",
        "      pred = pred.transpose(0, 1) # [batch_size, n_step+1(=6), n_class]\n",
        "      loss = 0\n",
        "      for i in range(len(dec_output_batch)):\n",
        "          # pred[i] : [n_step+1, n_class]\n",
        "          # dec_output_batch[i] : [n_step+1]\n",
        "          loss += criterion(pred[i], dec_output_batch[i])\n",
        "      if (epoch + 1) % 10 == 0:\n",
        "          print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n",
        "          \n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0010 cost = 198.478806\n",
            "Epoch: 0010 cost = 205.915100\n",
            "Epoch: 0010 cost = 202.548828\n",
            "Epoch: 0010 cost = 187.783661\n",
            "Epoch: 0020 cost = 195.139252\n",
            "Epoch: 0020 cost = 205.033325\n",
            "Epoch: 0020 cost = 167.998169\n",
            "Epoch: 0020 cost = 188.339142\n",
            "Epoch: 0030 cost = 177.108459\n",
            "Epoch: 0030 cost = 179.107635\n",
            "Epoch: 0030 cost = 173.080536\n",
            "Epoch: 0030 cost = 157.075287\n",
            "Epoch: 0040 cost = 143.527313\n",
            "Epoch: 0040 cost = 165.433548\n",
            "Epoch: 0040 cost = 154.091919\n",
            "Epoch: 0040 cost = 166.976456\n",
            "Epoch: 0050 cost = 127.797264\n",
            "Epoch: 0050 cost = 143.636932\n",
            "Epoch: 0050 cost = 151.816650\n",
            "Epoch: 0050 cost = 152.664948\n",
            "Epoch: 0060 cost = 141.294754\n",
            "Epoch: 0060 cost = 126.696762\n",
            "Epoch: 0060 cost = 133.156570\n",
            "Epoch: 0060 cost = 123.331192\n",
            "Epoch: 0070 cost = 137.483170\n",
            "Epoch: 0070 cost = 113.408058\n",
            "Epoch: 0070 cost = 123.395325\n",
            "Epoch: 0070 cost = 111.496490\n",
            "Epoch: 0080 cost = 113.469193\n",
            "Epoch: 0080 cost = 105.890053\n",
            "Epoch: 0080 cost = 105.625763\n",
            "Epoch: 0080 cost = 114.709244\n",
            "Epoch: 0090 cost = 95.989594\n",
            "Epoch: 0090 cost = 88.113815\n",
            "Epoch: 0090 cost = 97.411514\n",
            "Epoch: 0090 cost = 110.066574\n",
            "Epoch: 0100 cost = 82.741211\n",
            "Epoch: 0100 cost = 90.952065\n",
            "Epoch: 0100 cost = 95.205124\n",
            "Epoch: 0100 cost = 86.265938\n",
            "Epoch: 0110 cost = 79.070724\n",
            "Epoch: 0110 cost = 93.014351\n",
            "Epoch: 0110 cost = 78.267311\n",
            "Epoch: 0110 cost = 77.860909\n",
            "Epoch: 0120 cost = 76.646614\n",
            "Epoch: 0120 cost = 72.826103\n",
            "Epoch: 0120 cost = 74.779472\n",
            "Epoch: 0120 cost = 75.011658\n",
            "Epoch: 0130 cost = 71.772446\n",
            "Epoch: 0130 cost = 67.449348\n",
            "Epoch: 0130 cost = 67.224190\n",
            "Epoch: 0130 cost = 69.872826\n",
            "Epoch: 0140 cost = 59.617008\n",
            "Epoch: 0140 cost = 60.467991\n",
            "Epoch: 0140 cost = 71.886238\n",
            "Epoch: 0140 cost = 68.668076\n",
            "Epoch: 0150 cost = 64.102196\n",
            "Epoch: 0150 cost = 59.233833\n",
            "Epoch: 0150 cost = 63.989529\n",
            "Epoch: 0150 cost = 61.619625\n",
            "Epoch: 0160 cost = 54.116482\n",
            "Epoch: 0160 cost = 55.930202\n",
            "Epoch: 0160 cost = 64.657532\n",
            "Epoch: 0160 cost = 59.814236\n",
            "Epoch: 0170 cost = 53.328766\n",
            "Epoch: 0170 cost = 57.547665\n",
            "Epoch: 0170 cost = 59.863934\n",
            "Epoch: 0170 cost = 56.229706\n",
            "Epoch: 0180 cost = 48.245586\n",
            "Epoch: 0180 cost = 48.314423\n",
            "Epoch: 0180 cost = 56.638428\n",
            "Epoch: 0180 cost = 62.493935\n",
            "Epoch: 0190 cost = 52.733116\n",
            "Epoch: 0190 cost = 51.616161\n",
            "Epoch: 0190 cost = 47.803833\n",
            "Epoch: 0190 cost = 51.391045\n",
            "Epoch: 0200 cost = 51.158337\n",
            "Epoch: 0200 cost = 48.679363\n",
            "Epoch: 0200 cost = 51.091774\n",
            "Epoch: 0200 cost = 48.363941\n",
            "Epoch: 0210 cost = 41.102821\n",
            "Epoch: 0210 cost = 51.153957\n",
            "Epoch: 0210 cost = 49.513519\n",
            "Epoch: 0210 cost = 50.409348\n",
            "Epoch: 0220 cost = 40.209381\n",
            "Epoch: 0220 cost = 46.245705\n",
            "Epoch: 0220 cost = 46.581108\n",
            "Epoch: 0220 cost = 49.287590\n",
            "Epoch: 0230 cost = 45.413773\n",
            "Epoch: 0230 cost = 41.993114\n",
            "Epoch: 0230 cost = 44.578300\n",
            "Epoch: 0230 cost = 41.778786\n",
            "Epoch: 0240 cost = 43.164852\n",
            "Epoch: 0240 cost = 36.103920\n",
            "Epoch: 0240 cost = 46.404438\n",
            "Epoch: 0240 cost = 39.506386\n",
            "Epoch: 0250 cost = 38.703835\n",
            "Epoch: 0250 cost = 42.358891\n",
            "Epoch: 0250 cost = 38.052238\n",
            "Epoch: 0250 cost = 37.342186\n",
            "Epoch: 0260 cost = 36.600513\n",
            "Epoch: 0260 cost = 35.294342\n",
            "Epoch: 0260 cost = 41.638607\n",
            "Epoch: 0260 cost = 39.687813\n",
            "Epoch: 0270 cost = 36.340263\n",
            "Epoch: 0270 cost = 41.593071\n",
            "Epoch: 0270 cost = 36.225647\n",
            "Epoch: 0270 cost = 36.671555\n",
            "Epoch: 0280 cost = 35.999111\n",
            "Epoch: 0280 cost = 33.266689\n",
            "Epoch: 0280 cost = 36.292931\n",
            "Epoch: 0280 cost = 36.146652\n",
            "Epoch: 0290 cost = 32.632122\n",
            "Epoch: 0290 cost = 35.918259\n",
            "Epoch: 0290 cost = 32.623844\n",
            "Epoch: 0290 cost = 33.873211\n",
            "Epoch: 0300 cost = 34.376827\n",
            "Epoch: 0300 cost = 35.309429\n",
            "Epoch: 0300 cost = 34.104015\n",
            "Epoch: 0300 cost = 37.831375\n",
            "Epoch: 0310 cost = 31.727417\n",
            "Epoch: 0310 cost = 34.241371\n",
            "Epoch: 0310 cost = 34.778385\n",
            "Epoch: 0310 cost = 30.930859\n",
            "Epoch: 0320 cost = 31.059601\n",
            "Epoch: 0320 cost = 33.555805\n",
            "Epoch: 0320 cost = 31.291054\n",
            "Epoch: 0320 cost = 30.991535\n",
            "Epoch: 0330 cost = 30.441660\n",
            "Epoch: 0330 cost = 32.414371\n",
            "Epoch: 0330 cost = 29.990065\n",
            "Epoch: 0330 cost = 34.073227\n",
            "Epoch: 0340 cost = 32.445927\n",
            "Epoch: 0340 cost = 32.340794\n",
            "Epoch: 0340 cost = 29.734066\n",
            "Epoch: 0340 cost = 30.979805\n",
            "Epoch: 0350 cost = 28.007446\n",
            "Epoch: 0350 cost = 30.445845\n",
            "Epoch: 0350 cost = 31.257998\n",
            "Epoch: 0350 cost = 29.330139\n",
            "Epoch: 0360 cost = 30.736067\n",
            "Epoch: 0360 cost = 31.620628\n",
            "Epoch: 0360 cost = 30.041348\n",
            "Epoch: 0360 cost = 27.702930\n",
            "Epoch: 0370 cost = 28.819445\n",
            "Epoch: 0370 cost = 30.331902\n",
            "Epoch: 0370 cost = 28.704016\n",
            "Epoch: 0370 cost = 29.813190\n",
            "Epoch: 0380 cost = 29.916510\n",
            "Epoch: 0380 cost = 29.141268\n",
            "Epoch: 0380 cost = 27.918434\n",
            "Epoch: 0380 cost = 28.530592\n",
            "Epoch: 0390 cost = 30.085831\n",
            "Epoch: 0390 cost = 29.753422\n",
            "Epoch: 0390 cost = 29.293404\n",
            "Epoch: 0390 cost = 28.286865\n",
            "Epoch: 0400 cost = 27.744394\n",
            "Epoch: 0400 cost = 27.209242\n",
            "Epoch: 0400 cost = 27.618334\n",
            "Epoch: 0400 cost = 29.800207\n",
            "Epoch: 0410 cost = 28.589691\n",
            "Epoch: 0410 cost = 27.721912\n",
            "Epoch: 0410 cost = 27.792913\n",
            "Epoch: 0410 cost = 28.545677\n",
            "Epoch: 0420 cost = 27.344450\n",
            "Epoch: 0420 cost = 30.690016\n",
            "Epoch: 0420 cost = 28.490896\n",
            "Epoch: 0420 cost = 27.712616\n",
            "Epoch: 0430 cost = 28.768925\n",
            "Epoch: 0430 cost = 27.124365\n",
            "Epoch: 0430 cost = 28.102575\n",
            "Epoch: 0430 cost = 27.176737\n",
            "Epoch: 0440 cost = 27.388937\n",
            "Epoch: 0440 cost = 27.241520\n",
            "Epoch: 0440 cost = 26.678728\n",
            "Epoch: 0440 cost = 26.858282\n",
            "Epoch: 0450 cost = 26.400513\n",
            "Epoch: 0450 cost = 27.863127\n",
            "Epoch: 0450 cost = 28.050112\n",
            "Epoch: 0450 cost = 26.443388\n",
            "Epoch: 0460 cost = 26.480831\n",
            "Epoch: 0460 cost = 26.619991\n",
            "Epoch: 0460 cost = 28.305094\n",
            "Epoch: 0460 cost = 27.138512\n",
            "Epoch: 0470 cost = 26.458775\n",
            "Epoch: 0470 cost = 26.483372\n",
            "Epoch: 0470 cost = 30.365417\n",
            "Epoch: 0470 cost = 26.207924\n",
            "Epoch: 0480 cost = 26.490788\n",
            "Epoch: 0480 cost = 26.224443\n",
            "Epoch: 0480 cost = 30.314568\n",
            "Epoch: 0480 cost = 26.310982\n",
            "Epoch: 0490 cost = 27.486797\n",
            "Epoch: 0490 cost = 27.828587\n",
            "Epoch: 0490 cost = 26.139551\n",
            "Epoch: 0490 cost = 26.023756\n",
            "Epoch: 0500 cost = 25.950613\n",
            "Epoch: 0500 cost = 27.315931\n",
            "Epoch: 0500 cost = 25.867453\n",
            "Epoch: 0500 cost = 26.250820\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHQKT3J0sIwK"
      },
      "source": [
        "# Test\n",
        "def translate(word):\n",
        "    enc_input, dec_input, _ = make_data([[word, '']])\n",
        "    enc_input, dec_input = enc_input.to(device), dec_input.to(device)\n",
        "    # make hidden shape [num_layers * num_directions, batch_size, n_hidden]\n",
        "    # hidden = torch.zeros(1, 1, n_hidden).to(device)\n",
        "    # output = model(enc_input, hidden, dec_input)\n",
        "    output = model(enc_input, dec_input, 0)\n",
        "\n",
        "    # output : [n_step+1, batch_size, n_class]\n",
        "\n",
        "    predict = output.data.max(2, keepdim=True)[1] # select n_class dimension\n",
        "\n",
        "    decoded = [idx2word[i.item()] for i in predict]\n",
        "    translated = ' '.join(decoded[:decoded.index('<pad>')])\n",
        "    translated = translated.replace('<eos>','')\n",
        "    return translated"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xW35UtlPsQpg",
        "outputId": "b4950376-30ac-4348-d700-cfe5c448adce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# pairs[:5]\n",
        "for sent in pairs[:10]:\n",
        "    print(\"{} => {}\".format(sent[1], translate(sent[0])))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "love => ~ \n",
            "Starbucks I`m lovin` it => ~ I`m lovin` it \n",
            ".yummmmy! => ~ ! \n",
            "Hello, I see your online, can u talk to me pleeez!  From a fellow BAMF. lol => ~ , I see your online , can u talk to me pleeez ! From a fellow BAMF . lol \n",
            "fun => ~ \n",
            "it did, i didnt really watch it haha => ~ did , i didnt really watch it haha \n",
            "Wish => ~ \n",
            "Check this video out -- Bylaurenluke ~ Make up Launch~ They are here available now   http://tinyurl.com/cudamo => ~ this video out -- Bylaurenluke ~ Make up Launch~ They are here available now http : //tinyurl.com/cudamo \n",
            "Re-direct that energy into creating men`s jewelry.    And frequent walks to => ~ that energy into creating men`s jewelry . And frequent walks \n",
            "what`s wrong with dressing in fifties fashion? => ~ wrong with dressing in fifties fashion ? \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}