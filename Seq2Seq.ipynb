{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Seq2Seq.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPJnP876s5mVW+AiKFzx25J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yixish/NLPLearning/blob/master/Seq2Seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpH9RevmUlQv",
        "outputId": "cb577d12-92a7-4808-8bfd-f7f6a4cf5645",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')可以"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_p6iQc9Uu8-",
        "outputId": "09cbabf3-170c-4b7a-868b-f2dba3b0d2fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!/opt/bin/nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fri Nov  6 05:04:51 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 418.67       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   51C    P8     9W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Epwm883U2tg"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as Data\n",
        "import numpy as np \n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PSRqQsEYjLb",
        "outputId": "c9ee8e37-f4f2-4c46-84e9-d72f6c359505",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# f = open(\"/content/gdrive/My Drive/dataset/en-cn.csv\",\"r\")\n",
        "# pairs = f.readlines()\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"/content/gdrive/My Drive/dataset/en-cn.csv\",names=['id', 'en','cn'])\n",
        "ens = df['en'].values\n",
        "cns = df['cn'].values\n",
        "\n",
        "pairs = []\n",
        "num = 0\n",
        "for i in range(len(ens)):\n",
        "    if len(str(ens[i]).split()) < 100:\n",
        "        pairs.append([str(ens[i]),str(cns[i])])\n",
        "        num+=1\n",
        "    if num == 128:\n",
        "        break;\n",
        "\n",
        "# pairs = pairs[1:100]\n",
        "# pairs = [ s.replace('\\n',\"\").split('\\t') for s in pairs]\n",
        "pairs[:10]\n",
        "len(pairs)\n",
        "\n",
        "# df = pd.DataFrame(np.array(pairs[1:]))\n",
        "# df.to_csv(\"/home/kesci/work/sub.csv\",header=None)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "128"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pd1PfZg4a71o",
        "outputId": "2ada46e4-e9b1-462b-bf79-4b50f9ea2ca5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "en_chars = set()\n",
        "cn_chars = set()\n",
        "\n",
        "\n",
        "en_sents = []\n",
        "cn_sents = []\n",
        "\n",
        "for pair in pairs:\n",
        "    en_sent = pair[0]\n",
        "    cn_sent = pair[1]\n",
        "    en_sents.append(en_sent)\n",
        "    cn_sents.append(cn_sent)\n",
        "\n",
        "    for word in en_sent.split():\n",
        "        en_chars.add(word)\n",
        "\n",
        "    for word in cn_sent:\n",
        "        cn_chars.add(word)\n",
        "\n",
        "en_chars = list(en_chars)\n",
        "cn_chars = list(cn_chars)\n",
        "\n",
        "en_chars.extend(['<SOS>',\"<EOS>\",\"<PAD>\"])\n",
        "cn_chars.extend(['<SOS>',\"<EOS>\",\"<PAD>\"])\n",
        "\n",
        "en_chars = sorted(en_chars)\n",
        "cn_chars = sorted(cn_chars)\n",
        "\n",
        "num_encoder_tokens = len(en_chars)\n",
        "num_decoder_tokens = len(cn_chars)\n",
        "max_encoder_seq_len = max([ len(s.split()) for s in en_sents])\n",
        "max_decoder_seq_len = max([ len(s) for s in cn_sents])\n",
        "\n",
        "cn2idx = {n: i for i, n in enumerate(cn_chars)}\n",
        "en2idx = {n: i for i, n in enumerate(en_chars)}\n",
        "\n",
        "print('max_encoder_seq_len :',max_encoder_seq_len)\n",
        "print('max_decoder_seq_len :',max_decoder_seq_len)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "max_encoder_seq_len : 38\n",
            "max_decoder_seq_len : 49\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WMAZt0ia-gs"
      },
      "source": [
        "n_step = 55\n",
        "batch_size = 64\n",
        "n_hidden = 128\n",
        "\n",
        "def make_data(seq_data):\n",
        "    enc_input_all, dec_input_all, dec_output_all = [], [], []\n",
        "\n",
        "    for seq in seq_data:\n",
        "        # for i in range(2):\n",
        "            # seq[i] = seq[i] + ' <PAD>' * (n_step - len(seq[i])) \n",
        "        enc_input = [en2idx[n] for n in (seq[0] + ' <EOS>').split()]\n",
        "        \n",
        "        dec_input = []\n",
        "        dec_input.append(cn2idx['<SOS>'])\n",
        "        dec_input.extend([cn2idx[n] for n in seq[1]])\n",
        "\n",
        "        dec_output = [cn2idx[n] for n in seq[1]] \n",
        "        dec_output.append(cn2idx['<EOS>'])\n",
        "\n",
        "        for i in range(n_step - len(enc_input)):\n",
        "            enc_input.append(en2idx['<PAD>'])\n",
        "        for i in range(n_step - len(dec_input)):\n",
        "            dec_input.append(cn2idx['<PAD>'])\n",
        "        for i in range(n_step - len(dec_output)):\n",
        "            dec_output.append(cn2idx['<PAD>'])\n",
        "\n",
        "        enc_input_all.append(np.eye(num_encoder_tokens)[enc_input])\n",
        "        dec_input_all.append(np.eye(num_decoder_tokens)[dec_input])\n",
        "        dec_output_all.append(dec_output) \n",
        "\n",
        "\n",
        "    # make tensor\n",
        "    return torch.Tensor(enc_input_all), torch.Tensor(dec_input_all), torch.LongTensor(dec_output_all)\n",
        "\n",
        "enc_input_all, dec_input_all, dec_output_all = make_data(pairs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9xrlyxJbB-8"
      },
      "source": [
        "class TranslateDataSet(Data.Dataset):\n",
        "    def __init__(self, enc_input_all, dec_input_all, dec_output_all):\n",
        "        self.enc_input_all = enc_input_all\n",
        "        self.dec_input_all = dec_input_all\n",
        "        self.dec_output_all = dec_output_all\n",
        "    \n",
        "    def __len__(self): # return dataset size\n",
        "        return len(self.enc_input_all)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.enc_input_all[idx], self.dec_input_all[idx], self.dec_output_all[idx]\n",
        "\n",
        "loader = Data.DataLoader(TranslateDataSet(enc_input_all, dec_input_all, dec_output_all), batch_size, True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNT3H4ovNv_F"
      },
      "source": [
        "# Model\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = nn.GRU(input_size=num_encoder_tokens, hidden_size=n_hidden,dropout=0.5) # encoder\n",
        "        self.decoder = nn.GRU(input_size=num_decoder_tokens, hidden_size=n_hidden,dropout=0.5) # decoder\n",
        "        self.fc = nn.Linear(n_hidden, num_decoder_tokens)\n",
        "\n",
        "    def forward(self, enc_input, enc_hidden, dec_input):\n",
        "        # enc_input(=input_batch): [batch_size, n_step+1, n_class]\n",
        "        # dec_inpu(=output_batch): [batch_size, n_step+1, n_class]\n",
        "        enc_input = enc_input.transpose(0, 1) # enc_input: [n_step+1, batch_size, n_class]\n",
        "        dec_input = dec_input.transpose(0, 1) # dec_input: [n_step+1, batch_size, n_class]\n",
        "        # h_t : [num_layers(=1) * num_directions(=1), batch_size, n_hidden]\n",
        "        _, h_t= self.encoder(enc_input, enc_hidden)\n",
        "        # outputs : [n_step+1, batch_size, num_directions(=1) * n_hidden(=128)]\n",
        "        outputs, _ = self.decoder(dec_input,h_t)\n",
        "\n",
        "        model = self.fc(outputs) # model : [n_step+1, batch_size, n_class]\n",
        "        return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iX0xZfnN0h4",
        "outputId": "a3ea8376-240d-4710-d5e0-abf961920482",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = Seq2Seq().to(device)\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(5000):\n",
        "  for enc_input_batch, dec_input_batch, dec_output_batch in loader:\n",
        "      # make hidden shape [num_layers * num_directions, batch_size, n_hidden]\n",
        "      h_0 = torch.zeros(1, batch_size, n_hidden).to(device)\n",
        "\n",
        "      (enc_input_batch, dec_intput_batch, dec_output_batch) = (enc_input_batch.to(device), dec_input_batch.to(device), dec_output_batch.to(device))\n",
        "      # enc_input_batch : [batch_size, n_step+1, n_class]\n",
        "      # dec_intput_batch : [batch_size, n_step+1, n_class]\n",
        "      # dec_output_batch : [batch_size, n_step+1], not one-hot\n",
        "      pred = model(enc_input_batch, h_0, dec_intput_batch)\n",
        "      # pred : [n_step+1, batch_size, n_class]\n",
        "      pred = pred.transpose(0, 1) # [batch_size, n_step+1(=6), n_class]\n",
        "      loss = 0\n",
        "      for i in range(len(dec_output_batch)):\n",
        "          # pred[i] : [n_step+1, n_class]\n",
        "          # dec_output_batch[i] : [n_step+1]\n",
        "          loss += criterion(pred[i], dec_output_batch[i])\n",
        "      if (epoch + 1) % 500 == 0:\n",
        "          print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n",
        "          \n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:61: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0500 cost = 16.709499\n",
            "Epoch: 0500 cost = 18.907497\n",
            "Epoch: 1000 cost = 1.098630\n",
            "Epoch: 1000 cost = 1.082275\n",
            "Epoch: 1500 cost = 0.235056\n",
            "Epoch: 1500 cost = 0.208419\n",
            "Epoch: 2000 cost = 0.090194\n",
            "Epoch: 2000 cost = 0.093186\n",
            "Epoch: 2500 cost = 0.045772\n",
            "Epoch: 2500 cost = 0.043096\n",
            "Epoch: 3000 cost = 0.023712\n",
            "Epoch: 3000 cost = 0.022712\n",
            "Epoch: 3500 cost = 0.012100\n",
            "Epoch: 3500 cost = 0.013190\n",
            "Epoch: 4000 cost = 0.007097\n",
            "Epoch: 4000 cost = 0.007034\n",
            "Epoch: 4500 cost = 0.003724\n",
            "Epoch: 4500 cost = 0.004295\n",
            "Epoch: 5000 cost = 0.002152\n",
            "Epoch: 5000 cost = 0.002451\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvPpAgz3Qo9y"
      },
      "source": [
        "idx2cn = {i: n for i, n in enumerate(cn_chars)}\n",
        "# Test\n",
        "def translate(word):\n",
        "    enc_input, dec_input, _ = make_data([[word, '']])\n",
        "    enc_input, dec_input = enc_input.to(device), dec_input.to(device)\n",
        "    # make hidden shape [num_layers * num_directions, batch_size, n_hidden]\n",
        "    hidden = torch.zeros(1, 1, n_hidden).to(device)\n",
        "    output = model(enc_input, hidden, dec_input)\n",
        "    # output : [n_step+1, batch_size, n_class]\n",
        "\n",
        "    predict = output.data.max(2, keepdim=True)[1] # select n_class dimension\n",
        "\n",
        "\n",
        "    decoded = [idx2cn[i.item()] for i in predict]\n",
        "\n",
        "    translated = ''.join(decoded[:decoded.index('<PAD>')])\n",
        "\n",
        "    return translated"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQi4Co8x9nHs",
        "outputId": "063524a3-ff16-42ef-d884-c67c1e7de189",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "\n",
        "# pairs[:5]\n",
        "for sent in pairs:\n",
        "    print(\"{} => {}\".format(sent[1], translate(sent[0])))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "大卫.盖罗：这位是比尔.兰格， 我是大卫.盖罗。 => 大卫一一的都在这那。\n",
            "我们将用一些影片来讲述一些深海里的故事。 => 我们将用一中中一。。\n",
            "我们这有不少精彩的泰坦尼克的影片， 可惜您今天看不到。 => 我们这这有有有团儿里\n",
            "泰坦尼克号 是拿了不少票房冠军 但事实上它并不是关于海洋的最刺激的故事。 => 泰泰尼尼尼  的的是是－－－\n",
            "原因在于我们一直没把海洋当回事儿。 => 原真以喜一，的的动这\n",
            "大家想想，海洋占了地球面积的75％。 => 大家所，，的的都在虾太介。\n",
            "地球的大部分都是海水。 => 地的的的动动\n",
            "海洋的平均深度是两英里 => 海洋的的动动\n",
            "其实地球上最长的山脉都在海洋里。 => 其其这的的的这。。\n",
            "大部分的动物也都生活在海洋里。 => 大大一的的动\n",
            "大多数地震和火山喷发也都发生在海洋里 在海洋的最底部。 => 大多这这的的在这。。\n",
            "海洋里生物的多样性和密度要比 雨林带还高。 => 海洋这脉的的动动儿\n",
            "这儿基本上都没有被开发过，但是像这些美丽的景色， 它吸引着我们并被我们所熟知。 => 这儿会的的的都都科。\n",
            "但我想告诉你的是，当你站在海边时， 你面对的是一个完全陌生的世界。 => 但我一诉诉，，是是是－－－－－\n",
            "我们得用非常特殊的仪器 才能到达那个陌生的世界。 => 我们得些些的的的会会\n",
            "我们用的是深海潜水艇Alvin号和摄像机， 摄像机是比尔.兰格和索尼共同研发的。 => 我们用的的的动动动\n",
            "马塞尔.普鲁斯特说过：\"真正的探索之旅 不是为了新的发现，而是为了找到新的视角。\" => 马马塞想想所，的的－－－－－－\n",
            "这是一只水母。 => 这是一的的\n",
            "是我最喜欢的，因为它哪都能动。 => 是我最声一的的都－－样\n",
            "原来它是海洋中最长的生物。 => 原它它是海洋长。\n",
            "它可以伸展到150英尺长。 => 它可的的的动尺\n",
            "看到这些在动的东西了吗？ => 看看这不的在在动动\n",
            "我真喜欢这些东西。 => 我真真又一，度一科\n",
            "底下这些都是鱼饵。它们上上下下的浮动。 => 底里这些都都在这深。。\n",
            "还有这些摇晃着，旋转着的触角。 => 还有这这些虾动动\n",
            "这是一种群栖动物。 => 这是是是动动部\n",
            "其实它们都是由单独的动物， 结合在一起就成了这样大的一个生物。 => 其们会的的动动。。\n",
            "还有前面的这个是推进引擎。 它一会儿会用到它，还有一些光。 => 还还这些还动动的。\n",
            "如果你把所有的鱼类， 放在天平的一端，然后把所有水母状的动物 放在另一端，水母那边要重的多。 => 如我把把把想所所的的的－－－－至\n",
            "大多数的海洋生物都是由这类生物。 => 大多的的的的这长。\n",
            "这只是x翼死亡水母。 => 这只的的的的度层介\n",
            "它们用这种生物荧光来吸引伴侣， 食物和交流。 => 它们用用这里动的的。\n",
            "我们没办法从我们档案中的水母介绍起。 => 我们没没用管艇艇研各各\n",
            "它们大小不同，形状各异。 => 它它大的动动动\n",
            "而像这样的动物 就生活在 我们没有探索过的微重力的三维空间里。 => 而里这这这这。。。\n",
            "你一定听说过巨型章鱼之类的东西。 但像这样的动物可以伸张到140到160英尺长。 => 你一定的的，是是－－－至至\n",
            "它们还没有被研究透。 => 它们还些从中动动\n",
            "大卫：这也是其中之一，是我们另一个最爱，一个小的八爪鱼。 => 大卫：的的的的都都动。\n",
            "你真的能看穿它的脑袋。 => 你真螃是一一一一喷\n",
            "它的耳朵在上下摆动，还很优雅地向上游着。 => 它的的的动动\n",
            "我们能在不同深度，甚至最深处看到这样的动物。 => 我们能用海不一在\n",
            "它们有的几英尺长，有的几尺长。 => 它里有的的里这。\n",
            "它们有的会贴近潜水艇－－ 它们的眼睛会贴着潜水艇的窗口向里看。 => 它里有的的动动虾\n",
            "这里的世界中还有另一个世界， 我们将给您介绍两个。 => 这里的的这这。。\n",
            "像这一个，当我们到达了中海一下就能看见像这样子的物种。 => 像像一的都都这这。。\n",
            "它看起来有点像深海里的公鸡。 => 它看有的的的动动\n",
            "看这个，它看起来真的太正经了。 => 看这这的在在会在。\n",
            "还有这也是我的最爱之一。看这张脸！ => 还这这这个中动动里。\n",
            "你们看到的这些基本上都是科学数据。 => 你们当们的的这这们太\n",
            "它们都是我们为了科学研究而收集的影片。 => 它这都都从中团里\n",
            "这个是比尔正在做的， 为了让科学家们看到这些第一手材料 这些在它们生存的环境中获取的。 => 这这都都都为为科。。\n",
            "他们不会用鱼网来捕这些生物。 => 他们不们海洋中里\n",
            "他们会在它们的世界中观察它们。 => 他们会在在中中。。\n",
            "我们会用一个控制杆， 在地面上我们只用坐在电脑前， 移动控制杆就能环游地球了。 => 我们会用在在这动深。\n",
            "现在我们看一个海中央的山脊。 一个40，000英里长的山脉。 => 现现我们的的都这这。。\n",
            "这些山脉顶端的平均深度都有1.5英里。 => 这些这顶顶长。。\n",
            "我们已走遍了大西洋－－那就是山脊， 现在我们要穿过加勒比海，中美洲， 最后到达太平洋，北纬九度。 => 我们已用用的中都的\n",
            "我们用声纳来制作这些山脊的地图， 这是这些山脊中的一个。 => 我们用用用用一在这。\n",
            "我们现在向右转过一个悬崖。 => 我们我们们用用中中中\n",
            "这些山脉两侧山谷的高度 大多数都比阿尔卑斯山脉还要高。 => 这些山脉两动动点里。\n",
            "这里还有成千上万的山脉不在地图上。 => 这里还这这里。。\n",
            "这是一个火山脊。 => 这是一个大水里。\n",
            "我们现在往更深处走。 => 我们现们用中中中\n",
            "最终我们会看到像这样的东西。 => 最终最会一的中都－－。\n",
            "这是最具代表性的一个机器人－他叫杰森。 => 这是是是的的中都。。\n",
            "你会坐在这样的一间房间里， 用遥控杆和耳机来这样驾驶机器人 同时机器人在海底行走。 => 你会坐会的的都动动。。\n",
            "在伍兹霍尔，我们还希望和我们的合伙人 把这个虚拟的世界－－ 这个未曾背发掘的地带－带回实验室里。 => 在伍想想，，，是是一地地地地\n",
            "因为我们现在看到的都是些点滴片断。 => 因因：们看的的都都这\n",
            "我们获取了一些声音，一些影象， 或是一些图片，或是一些化学成分 －－但我们从没把它们放在一块来看。 => 我们最在的的都这这在。\n",
            "这里是比尔的相机真正出彩的地方。 => 这里是是这点动动动\n",
            "我们叫这个热泉喷出口。 => 我们叫的的这中。。\n",
            "您现在看到的是一团密度很高的 强硫化氫液体 从海底的火山中轴喷出。 => 您终最看生的的在这太。。\n",
            "有时可以达到600到700华氏度。 => 有有的的的正77介\n",
            "这些都是海水中的液体－ 一点五英里，两英里，三英里深。 => 这这这顶动动\n",
            "六七十年代时我们只知道这是一座火山。 => 六六：：：看看的的的都－－\n",
            "我们真没想到硫化氢的含量会这么高。 => 我们真们们潜潜潜潜潜潜潜潜殊表表表表高高他\n",
            "我们当初根本不知道这些是什么，我们叫它们烟囱。 => 我们当们们们们们们们们们们的的的\n",
            "这是这些热液出口中的一个。 => 这是这是热动水。\n",
            "300多度的液体从地里涌出。 => 3里有的动\n",
            "我们两侧的山脊都比阿尔卑斯山高， 所以说这的地形是惊人的。 => 我们两侧不都这们们。\n",
            "比尔：这些白色的物质是一种细菌 它能在180度的高温下生存。 => 比尔：一看的的都都这。。\n",
            "我们开始想 它为什么在那里？ => 我我开开些中中中\n",
            "我们现在知道它可能是来自地球内部。 => 我们现们们知知一一的\n",
            "它不仅从地球里出来－－ 它的生物源来自于火山活动－－ 但这些细菌供养群居在这里的生物。 => 它不里大的的的。。\n",
            "这里的压力是平均每平方英尺4000磅。 => 这里这的的动动动\n",
            "离地表一点五英里，两三英里深 没有任何阳光的照射。 => 离不这不的在这动儿。\n",
            "支撑所有的生命形式的能量 都来自地球内部－那些化学合成物。 => 支支：：想能看的的的动动－\n",
            "你可以看到这的生物密度是多大。 => 你可一的的的在动动\n",
            "它们被叫做管道蠕虫。 => 它们被被水水艇的高\n",
            "比尔：这些蠕虫没有消化系统。它们没有嘴。 => 比们最的的的做为为们\n",
            "但它们有两种腮组织。 => 但们它里大的里里。。\n",
            "大卫：你可以看到－这是一只生活在这里的螃蟹。 => 大卫：：一的的的是是地地。\n",
            "它能钳住蠕虫的一小部分。 => 它能钳热热的的的7\n",
            "它们通常一碰到螃蟹就缩回去。 => 它它通小鱼中的。\n",
            "恩！厉害。 => 恩恩这的的\n",
            "所以，一旦螃蟹碰到它们 它们就缩回壳里，就像指甲。 => 所所，的的的都它会\n",
            "这样的故事被一一展开 我们只是开始对它们有些认识 全都靠这个新的摄影技术。 => 这样会的动动动。\n",
            "比尔：这些蠕虫都生活在这样极端的温度下。 => 比们一的的的都这这介。\n",
            "它们的脚有大概200度 它们的头是3度左右， 这就像是你的手放在沸水中而你的脚在冰水里。 => 它它的的的动动动\n",
            "它们就是这样生活的。 => 它们就这\n",
            "大卫：这是一只雌性蠕虫。 => 大卫的的的的都这样太。\n",
            "这只是雄性的。 => 这只的只水水\n",
            "你看着。用不着多久 它们两就会出现在这－开始打斗。 => 你看着用不都这这动。\n",
            "你所看到的都是在深海最漆黑的地方进行的， => 你所所看的的都都这太。\n",
            "除了我们带来过的光线之外没有任何其他的光线。 => 除除这这这这这这虾动\n",
            "看这里。 => 看这里\n",
            "在过去的潜水系列中有一次 我们在这个区域里找到200种物种。 198种都是新的物种。 => 在过去一看的的都这这。\n",
            "比尔：对生物学家来说最大的问题之一 是在这些地点工作，更不用说采集物种。 => 比尔：想想知，些是是地地地地\n",
            "因为采集上来它们就会破裂， 所以这些影像对科学来说是至关重要的。 => 因采着的的的都－－\n",
            "大卫：这是两条在两英里深处的章鱼。 => 大卫一的的的都这这。。\n",
            "直到现在我们看到的都来自太平洋。 => 直直现现现们看的的都都－\n",
            "这些是来自大西洋。而且更深。 => 这这是有动动动动。\n",
            "你可以看到这只虾正在骚扰这个可怜的小东西 它会用它的爪子反抗。嗷！ => 你可的看的的这这饵。\n",
            "类似的故事一只在发生。 => 类似一的的水\n",
            "它们在这只蟹背上能获得的是 这种很奇怪的细菌类的食物 它生长在这些动物的背上。 => 它它在这洋里长。。\n",
            "而这些虾 其实正在这些动物的背上收割细菌。 => 而这这这中里。。\n",
            "这些螃蟹可不喜欢这样。 => 这些螃用用中一深\n",
            "这些蟹背上的长的细丝 是由这种细菌组成的 => 这些些中动动\n",
            "所以细菌可以在蟹背上长出毛来。 => 所以通一，的动在\n",
            "现在你又看到了同样的在发生。 => 现现你又又，的一喷这\n",
            "这个红点是潜水艇Alvin号腹下的镭射灯 能让我们感觉到我们离喷出口有多远。 => 这个个个都地们们们。\n",
            "这些都是虾。 => 这这这虾\n",
            "你可以看到热液从这儿，这儿还有这儿冒出来。 => 你可以的的的的这\n",
            "它们会粘在岩石的表面 并从岩石表面把细菌扒下来。 => 它们会会虾热饵虾\n",
            "这里有一个很小的出口在那个很大的柱子上。 => 这里有有长这。\n",
            "这些柱子可以有几层楼高。 => 这些柱的大水。\n",
            "它们毫不需要阳光。 => 它们毫毫大\n",
            "比尔：你能看到这些虾的背上有v形的记号吗？ => 比尔：想想想看知知些的的动地地\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTg5gDTIFeG_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}